{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9868125,"sourceType":"datasetVersion","datasetId":6057270}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\nfrom transformers import GPT2TokenizerFast, AutoTokenizer\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:04:09.405753Z","iopub.execute_input":"2024-11-11T16:04:09.406196Z","iopub.status.idle":"2024-11-11T16:04:09.411704Z","shell.execute_reply.started":"2024-11-11T16:04:09.406157Z","shell.execute_reply":"2024-11-11T16:04:09.410472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef train_tokenizer(data_list, vocab_size=32768, model_name=\"GPT2ByteLevelTokenizer\"):\n    \"\"\"\n    Train a GPT-2 Byte-Level Tokenizer for Hindi data.\n\n    Args:\n    - data_list: A list of sentences for training.\n    - vocab_size: The vocabulary size for the tokenizer.\n    - model_name: The name to save the trained tokenizer model.\n    \"\"\"\n    # Initialize GPT-2 Byte-Level Tokenizer\n    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\n    # Add special tokens\n    tokenizer.add_special_tokens({\n        \"bos_token\": \"<sos>\",\n        \"eos_token\": \"<end_of_sen>\",\n        \"pad_token\": \"<pad>\",\n        \"unk_token\": \"<unk>\",\n        \"additional_special_tokens\": [\"<user>\", \"<assistant>\"]\n    })\n\n    # Train the tokenizer on the data\n    tokenizer.train_new_from_iterator(data_list, vocab_size=vocab_size)\n\n    # Save the tokenizer model\n    tokenizer.save_pretrained(model_name)\n    print(f\"GPT-2 Byte-Level Tokenizer saved to: {model_name}\")\n\n    # Save the vocabulary as a JSON file\n    vocab = tokenizer.get_vocab()\n    vocab_file = os.path.join(model_name, \"GPT2ByteLevelTokenizer.json\")\n    with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(vocab, f, ensure_ascii=False, indent=4)\n    print(f\"Vocabulary saved to: {vocab_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:32:34.317560Z","iopub.status.idle":"2024-11-11T16:32:34.318002Z","shell.execute_reply.started":"2024-11-11T16:32:34.317759Z","shell.execute_reply":"2024-11-11T16:32:34.317779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(file_path):\n    data = []\n    with open(file_path,'r') as f:\n        for x in f:\n            data.append(x)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:04:10.742741Z","iopub.execute_input":"2024-11-11T16:04:10.743676Z","iopub.status.idle":"2024-11-11T16:04:10.748712Z","shell.execute_reply.started":"2024-11-11T16:04:10.743626Z","shell.execute_reply":"2024-11-11T16:04:10.747478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_fertility_score(tokenizer, data_list):\n    \"\"\"\n    Calculate the overall fertility score for the entire dataset based on the tokenizer output.\n\n    Args:\n    - tokenizer: The trained tokenizer object.\n    - data_list: A list of sentences for which fertility scores are calculated.\n\n    Returns:\n    - A single fertility score for the entire dataset.\n    \"\"\"\n    total_word_count = 0\n    total_token_count = 0\n    max_length = 1024  # Maximum sequence length\n    for sentence in data_list:\n        input_ids = tokenizer.encode(sentence, max_length=max_length, truncation=True, clean_up_tokenization_spaces=False)\n        word_count = len(sentence.split())\n        token_count = len(input_ids)\n        total_word_count += word_count\n        total_token_count += token_count\n\n    fertility_score = total_token_count / total_word_count if total_word_count > 0 else 0\n    return fertility_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:04:11.898686Z","iopub.execute_input":"2024-11-11T16:04:11.899114Z","iopub.status.idle":"2024-11-11T16:04:11.905772Z","shell.execute_reply.started":"2024-11-11T16:04:11.899077Z","shell.execute_reply":"2024-11-11T16:04:11.904545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the input folder containing the CSV files\ninput_folder = '/kaggle/input/hindi-dataset-10k-files/'  # Folder containing Hindi .csv files\n\n\noutput_file = \"fertility_score_GPT2ByteLevelTokenizer.csv\"  # Output file for fertility scores\n\n# Collect data from all CSV files in the input folder\nhindi_data = []\nerr_cnt = 0\nfor filename in tqdm(os.listdir(input_folder), desc=\"Processing files\"):\n    try:\n        if filename.endswith(\".csv\"):\n            file_path = os.path.join(input_folder, filename)\n            # Process each file and append valid sentences\n            file_data = preprocess_text(file_path)\n            if file_data:  # Only extend if the file has valid sentences\n                hindi_data.extend(file_data)\n    except Exception as e:\n        err_cnt+=1\n\nprint('error occured in files: ',err_cnt)\n\nprint(f\"Total sentences for training: {len(hindi_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:32:34.315334Z","iopub.status.idle":"2024-11-11T16:32:34.315769Z","shell.execute_reply.started":"2024-11-11T16:32:34.315568Z","shell.execute_reply":"2024-11-11T16:32:34.315590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Train the tokenizer on the collected data\nif hindi_data:\n    model_name = \"GPT2ByteLevelTokenizer\"\n    train_tokenizer(hindi_data, vocab_size=32768, model_name=model_name)\n\n    # Load the trained tokenizer for testing\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Calculate the overall fertility score\n    fertility_score = calculate_fertility_score(tokenizer, hindi_data)\n    \n    # Save the fertility score to a CSV file\n    with open(output_file, \"w\", encoding='utf-8-sig') as f:\n        f.write(\"Fertility Score\\n\")\n        f.write(f\"{fertility_score}\\n\")\n    print(f\"Fertility score saved to '{output_file}'\")\n\n    # Display the fertility score\n    print(f\"Overall Fertility Score: {fertility_score}\")\nelse:\n    print(\"No valid data found to train the tokenizer.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:05:08.959510Z","iopub.execute_input":"2024-11-11T16:05:08.959932Z","iopub.status.idle":"2024-11-11T16:32:32.339839Z","shell.execute_reply.started":"2024-11-11T16:05:08.959894Z","shell.execute_reply":"2024-11-11T16:32:32.338441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}