{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9868125,"sourceType":"datasetVersion","datasetId":6057270}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\nfrom tokenizers import SentencePieceBPETokenizer\nfrom transformers import PreTrainedTokenizerFast, AutoTokenizer\nimport spacy\nimport json\n# from tokenizers import BertWordPieceTokenizer\n# from tokenizers import ByteLevelBPETokenizer\n# from tokenizers import SentencePieceUnigramTokenizer\n# from transformers import GPT2TokenizerFast, AutoTokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function for tokenizer \ndef train_tokenizer(data_list, vocab_size=32000, model_name=\"SentencePieceBPETokenizer\"):\n    \"\"\"\n    Train a SentencePiece BPE tokenizer for Hindi data.\n\n    Args:\n    - data_list: A list of sentences for training.\n    - vocab_size: The vocabulary size for the tokenizer.\n    - model_name: The name to save the trained tokenizer model.\n    \"\"\"\n    # Define special tokens\n    bos_tok = \"<sos>\"\n    eos_tok = \"<end_of_sen>\"\n\n    # Special characters list could include Hindi numerals or commonly used punctuation\n    special_char = [\"०\", \"१\", \"२\", \"३\", \"४\", \"५\", \"६\", \"७\", \"८\", \"९\", \"।\", \"॥\"]\n\n    # Initialize the tokenizer\n    tokenizer = SentencePieceBPETokenizer()\n\n    # Train the tokenizer on the Hindi data\n    tokenizer.train_from_iterator(\n        data_list,\n        vocab_size=vocab_size,\n        min_frequency=5,\n        special_tokens=[\"<pad>\", \"<unk>\", bos_tok, eos_tok, \"<user>\", \"<assistant>\"] + special_char,\n        show_progress=True,\n    )\n\n    # Wrap the tokenizer with Hugging Face’s PreTrainedTokenizerFast\n    transformer_tokenizer = PreTrainedTokenizerFast(\n        tokenizer_object=tokenizer,\n        bos_token=bos_tok,\n        eos_token=eos_tok,\n        unk_token=\"<unk>\",\n        pad_token=\"<pad>\",\n        mask_token=\"<mask>\",\n        padding_side=\"left\",\n        truncation_side=\"right\",\n        additional_special_tokens=[\"<user>\", \"<assistant>\"],\n        clean_up_tokenization_spaces=False,\n    )\n\n    # Save the tokenizer model\n    transformer_tokenizer.save_pretrained(model_name)\n    print(f\"Tokenizer saved to: {model_name}\")\n\n    # Save the vocabulary as a JSON file\n    vocab = transformer_tokenizer.get_vocab()\n    vocab_file = os.path.join(model_name, \"SentencePieceBPETokenizer.json\")\n    with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(vocab, f, ensure_ascii=False, indent=4)\n    print(f\"Vocabulary saved to: {vocab_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:25:27.509066Z","iopub.execute_input":"2024-11-11T12:25:27.509493Z","iopub.status.idle":"2024-11-11T12:25:27.522629Z","shell.execute_reply.started":"2024-11-11T12:25:27.509455Z","shell.execute_reply":"2024-11-11T12:25:27.521372Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def preprocess_text(file_path):\n    data = []\n    with open(file_path,'r') as f:\n        for x in f:\n            data.append(x)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:25:33.320695Z","iopub.execute_input":"2024-11-11T12:25:33.321182Z","iopub.status.idle":"2024-11-11T12:25:33.327403Z","shell.execute_reply.started":"2024-11-11T12:25:33.321136Z","shell.execute_reply":"2024-11-11T12:25:33.326197Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def calculate_fertility_score(tokenizer, data_list):\n    \"\"\"\n    Calculate the overall fertility score for the entire dataset based on the tokenizer output.\n\n    Args:\n    - tokenizer: The trained tokenizer object.\n    - data_list: A list of sentences for which fertility scores are calculated.\n\n    Returns:\n    - A single fertility score for the entire dataset.\n    \"\"\"\n    total_word_count = 0\n    total_token_count = 0\n    \n    # Calculate word and token counts for each sentence\n    for sentence in data_list:\n        # Tokenize the sentence\n        input_ids = tokenizer.encode(sentence)\n        word_count = len(sentence.split())\n        token_count = len(input_ids)\n\n        total_word_count += word_count\n        total_token_count += token_count\n\n    # Calculate the overall fertility score\n    fertility_score = total_token_count / total_word_count if total_word_count > 0 else 0\n    return fertility_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:25:36.328723Z","iopub.execute_input":"2024-11-11T12:25:36.329203Z","iopub.status.idle":"2024-11-11T12:25:36.336948Z","shell.execute_reply.started":"2024-11-11T12:25:36.329150Z","shell.execute_reply":"2024-11-11T12:25:36.335589Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Define the input folder containing the CSV files\ninput_folder = '/kaggle/input/hindi-dataset-10k-files/'  # Folder containing Hindi .csv files\n\n\noutput_file = \"fertility_score_SentencePieceBPETokenizer.csv\"  # Output file for fertility scores\n\n# Collect data from all CSV files in the input folder\nhindi_data = []\nerr_cnt = 0\nfor filename in tqdm(os.listdir(input_folder), desc=\"Processing files\"):\n    try:\n        if filename.endswith(\".csv\"):\n            file_path = os.path.join(input_folder, filename)\n            # Process each file and append valid sentences\n            file_data = preprocess_text(file_path)\n            if file_data:  # Only extend if the file has valid sentences\n                hindi_data.extend(file_data)\n    except Exception as e:\n        err_cnt+=1\n\nprint('error occured in files: ',err_cnt)\n\nprint(f\"Total sentences for training: {len(hindi_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:27:23.768642Z","iopub.execute_input":"2024-11-11T12:27:23.769650Z","iopub.status.idle":"2024-11-11T12:28:07.218361Z","shell.execute_reply.started":"2024-11-11T12:27:23.769602Z","shell.execute_reply":"2024-11-11T12:28:07.217260Z"}},"outputs":[{"name":"stderr","text":"Processing files: 100%|██████████| 3000/3000 [00:43<00:00, 69.62it/s]","output_type":"stream"},{"name":"stdout","text":"error occured in files:  0\nTotal sentences for training: 4707630\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\n# Train the tokenizer on the collected data\nif hindi_data:\n    model_name = \"SentencePieceBPETokenizer\"\n    train_tokenizer(hindi_data, vocab_size=32000, model_name=model_name)\n\n    # Load the trained tokenizer for testing\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Calculate the overall fertility score\n    fertility_score = calculate_fertility_score(tokenizer, hindi_data)\n    \n    # Save the fertility score to a CSV file\n    with open(output_file, \"w\", encoding='utf-8-sig') as f:\n        f.write(\"Fertility Score\\n\")\n        f.write(f\"{fertility_score}\\n\")\n    print(f\"Fertility score saved to '{output_file}'\")\n\n    # Display the fertility score\n    print(f\"Overall Fertility Score: {fertility_score}\")\nelse:\n    print(\"No valid data found to train the tokenizer.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:28:45.516997Z","iopub.execute_input":"2024-11-11T12:28:45.517439Z","iopub.status.idle":"2024-11-11T12:41:54.207549Z","shell.execute_reply.started":"2024-11-11T12:28:45.517401Z","shell.execute_reply":"2024-11-11T12:41:54.206377Z"}},"outputs":[{"name":"stdout","text":"\n\n\nTokenizer saved to: SentencePieceBPETokenizer\nVocabulary saved to: SentencePieceBPETokenizer/SentencePieceBPETokenizer.json\nFertility score saved to 'fertility_score_SentencePieceBPETokenizer.csv'\nOverall Fertility Score: 1.2406445450652963\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}