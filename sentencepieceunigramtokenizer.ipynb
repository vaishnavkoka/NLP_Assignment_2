{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9868125,"sourceType":"datasetVersion","datasetId":6057270}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\nfrom tokenizers import SentencePieceUnigramTokenizer\nfrom transformers import PreTrainedTokenizerFast, AutoTokenizer\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:04:36.368079Z","iopub.execute_input":"2024-11-11T16:04:36.368695Z","iopub.status.idle":"2024-11-11T16:04:36.374643Z","shell.execute_reply.started":"2024-11-11T16:04:36.368650Z","shell.execute_reply":"2024-11-11T16:04:36.373365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_tokenizer(data_list, vocab_size=32768, model_name=\"SentencePieceUnigramTokenizer\"):\n    \"\"\"\n    Train a SentencePiece Unigram tokenizer for Hindi data.\n\n    Args:\n    - data_list: A list of sentences for training.\n    - vocab_size: The vocabulary size for the tokenizer.\n    - model_name: The name to save the trained tokenizer model.\n    \"\"\"\n    # Define special tokens\n    bos_tok = \"<sos>\"\n    eos_tok = \"<end_of_sen>\"\n\n    # Special characters list could include Hindi numerals or commonly used punctuation\n    special_char = [\"०\", \"१\", \"२\", \"३\", \"४\", \"५\", \"६\", \"७\", \"८\", \"९\", \"।\", \"॥\"]\n\n    # Initialize the tokenizer\n    tokenizer = SentencePieceUnigramTokenizer()\n\n    # Train the tokenizer on the Hindi data\n    tokenizer.train_from_iterator(\n        data_list,\n        vocab_size=vocab_size,\n        special_tokens=[\"<pad>\", \"<unk>\", bos_tok, eos_tok, \"<user>\", \"<assistant>\"] + special_char,\n        show_progress=True,\n    )\n\n    # Wrap the tokenizer with Hugging Face’s PreTrainedTokenizerFast\n    transformer_tokenizer = PreTrainedTokenizerFast(\n        tokenizer_object=tokenizer,\n        bos_token=bos_tok,\n        eos_token=eos_tok,\n        unk_token=\"<unk>\",\n        pad_token=\"<pad>\",\n        mask_token=\"<mask>\",\n        padding_side=\"left\",\n        truncation_side=\"right\",\n        additional_special_tokens=[\"<user>\", \"<assistant>\"],\n        clean_up_tokenization_spaces=False,\n    )\n\n    # Save the tokenizer model\n    transformer_tokenizer.save_pretrained(model_name)\n    print(f\"Tokenizer saved to: {model_name}\")\n\n    # Save the vocabulary as a JSON file\n    vocab = transformer_tokenizer.get_vocab()\n    vocab_file = os.path.join(model_name, \"SentencePieceUnigramTokenizer.json\")\n    with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(vocab, f, ensure_ascii=False, indent=4)\n    print(f\"Vocabulary saved to: {vocab_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:04:37.491392Z","iopub.execute_input":"2024-11-11T16:04:37.491815Z","iopub.status.idle":"2024-11-11T16:04:37.504136Z","shell.execute_reply.started":"2024-11-11T16:04:37.491775Z","shell.execute_reply":"2024-11-11T16:04:37.501608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(file_path):\n    data = []\n    with open(file_path,'r') as f:\n        for x in f:\n            data.append(x)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:04:38.458353Z","iopub.execute_input":"2024-11-11T16:04:38.458786Z","iopub.status.idle":"2024-11-11T16:04:38.464750Z","shell.execute_reply.started":"2024-11-11T16:04:38.458745Z","shell.execute_reply":"2024-11-11T16:04:38.463542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_fertility_score(tokenizer, data_list):\n    \"\"\"\n    Calculate the overall fertility score for the entire dataset based on the tokenizer output.\n\n    Args:\n    - tokenizer: The trained tokenizer object.\n    - data_list: A list of sentences for which fertility scores are calculated.\n\n    Returns:\n    - A single fertility score for the entire dataset.\n    \"\"\"\n    total_word_count = 0\n    total_token_count = 0\n\n    # Calculate word and token counts for each sentence\n    for sentence in data_list:\n        # Tokenize the sentence\n        input_ids = tokenizer.encode(sentence)\n        word_count = len(sentence.split())\n        token_count = len(input_ids)\n\n        total_word_count += word_count\n        total_token_count += token_count\n\n    # Calculate the overall fertility score\n    fertility_score = total_token_count / total_word_count if total_word_count > 0 else 0\n    return fertility_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:04:39.365259Z","iopub.execute_input":"2024-11-11T16:04:39.366343Z","iopub.status.idle":"2024-11-11T16:04:39.372781Z","shell.execute_reply.started":"2024-11-11T16:04:39.366290Z","shell.execute_reply":"2024-11-11T16:04:39.371549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the input folder containing the CSV files\ninput_folder = '/kaggle/input/hindi-dataset-10k-files/'  # Folder containing Hindi .csv files\n\n\noutput_file = \"fertility_score_SentencePieceUnigramTokenizer.csv\"  # Output file for fertility scores\n\n# Collect data from all CSV files in the input folder\nhindi_data = []\nerr_cnt = 0\nfor filename in tqdm(os.listdir(input_folder), desc=\"Processing files\"):\n    try:\n        if filename.endswith(\".csv\"):\n            file_path = os.path.join(input_folder, filename)\n            # Process each file and append valid sentences\n            file_data = preprocess_text(file_path)\n            if file_data:  # Only extend if the file has valid sentences\n                hindi_data.extend(file_data)\n    except Exception as e:\n        err_cnt+=1\n\nprint('error occured in files: ',err_cnt)\n\nprint(f\"Total sentences for training: {len(hindi_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:04:41.335790Z","iopub.execute_input":"2024-11-11T16:04:41.336229Z","iopub.status.idle":"2024-11-11T16:05:20.271828Z","shell.execute_reply.started":"2024-11-11T16:04:41.336184Z","shell.execute_reply":"2024-11-11T16:05:20.270568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the tokenizer on the collected data\nif hindi_data:\n    model_name = \"SentencePieceUnigramTokenizer\"\n    train_tokenizer(hindi_data, vocab_size=32768, model_name=model_name)\n\n    # Load the trained tokenizer for testing\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Calculate the overall fertility score\n    fertility_score = calculate_fertility_score(tokenizer, hindi_data)\n    \n    # Save the fertility score to a CSV file\n    with open(output_file, \"w\", encoding='utf-8-sig') as f:\n        f.write(\"Fertility Score\\n\")\n        f.write(f\"{fertility_score}\\n\")\n    print(f\"Fertility score saved to '{output_file}'\")\n\n    # Display the fertility score\n    print(f\"Overall Fertility Score: {fertility_score}\")\nelse:\n    print(\"No valid data found to train the tokenizer.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:05:22.436553Z","iopub.execute_input":"2024-11-11T16:05:22.436959Z","iopub.status.idle":"2024-11-11T16:24:09.577146Z","shell.execute_reply.started":"2024-11-11T16:05:22.436921Z","shell.execute_reply":"2024-11-11T16:24:09.575961Z"}},"outputs":[],"execution_count":null}]}