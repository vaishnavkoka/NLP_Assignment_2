{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9868125,"sourceType":"datasetVersion","datasetId":6057270}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import PreTrainedTokenizerFast, AutoTokenizer\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:03:33.147168Z","iopub.execute_input":"2024-11-11T16:03:33.147835Z","iopub.status.idle":"2024-11-11T16:03:33.154433Z","shell.execute_reply.started":"2024-11-11T16:03:33.147787Z","shell.execute_reply":"2024-11-11T16:03:33.153035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef train_tokenizer(data_list, vocab_size=32768, model_name=\"BertWordPieceTokenizer\"):\n    \"\"\"\n    Train a BertWordPiece tokenizer for Hindi data.\n\n    Args:\n    - data_list: A list of sentences for training.\n    - vocab_size: The vocabulary size for the tokenizer.\n    - model_name: The name to save the trained tokenizer model.\n    \"\"\"\n    # Define special tokens\n    bos_tok = \"<sos>\"\n    eos_tok = \"<end_of_sen>\"\n\n    # Special characters list could include Hindi numerals or commonly used punctuation\n    special_char = [\"०\", \"१\", \"२\", \"३\", \"४\", \"५\", \"६\", \"७\", \"८\", \"९\", \"।\", \"॥\"]\n\n    # Initialize the tokenizer\n    tokenizer = BertWordPieceTokenizer()\n\n    # Train the tokenizer on the Hindi data\n    tokenizer.train_from_iterator(\n        data_list,\n        vocab_size=vocab_size,\n        min_frequency=5,\n        special_tokens=[\"<pad>\", \"<unk>\",\"[UNK]\", bos_tok, eos_tok, \"<user>\", \"<assistant>\"] + special_char,\n        show_progress=True,\n    )\n\n    # Wrap the tokenizer with Hugging Face’s PreTrainedTokenizerFast\n    transformer_tokenizer = PreTrainedTokenizerFast(\n        tokenizer_object=tokenizer,\n        bos_token=bos_tok,\n        eos_token=eos_tok,\n        unk_token=\"[UNK]\",\n        pad_token=\"<pad>\",\n        mask_token=\"<mask>\",\n        padding_side=\"left\",\n        truncation_side=\"right\",\n        additional_special_tokens=[\"<user>\", \"<assistant>\"],\n        clean_up_tokenization_spaces=False,\n    )\n\n    # Save the tokenizer model\n    tokenizer_folder = model_name  # Folder where the tokenizer will be saved\n    os.makedirs(tokenizer_folder, exist_ok=True)\n    transformer_tokenizer.save_pretrained(tokenizer_folder)\n    print(f\"Tokenizer saved to: {tokenizer_folder}\")\n\n    # Save the vocabulary as a JSON file\n    vocab = transformer_tokenizer.get_vocab()\n    vocab_file = os.path.join(tokenizer_folder, \"BertWordPieceTokenizer.json\")\n    with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(vocab, f, ensure_ascii=False, indent=4)\n    print(f\"Vocabulary saved to: {vocab_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:03:33.786576Z","iopub.execute_input":"2024-11-11T16:03:33.786997Z","iopub.status.idle":"2024-11-11T16:03:33.800398Z","shell.execute_reply.started":"2024-11-11T16:03:33.786955Z","shell.execute_reply":"2024-11-11T16:03:33.798953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(file_path):\n    data = []\n    with open(file_path,'r') as f:\n        for x in f:\n            data.append(x)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:03:34.346844Z","iopub.execute_input":"2024-11-11T16:03:34.347322Z","iopub.status.idle":"2024-11-11T16:03:34.353823Z","shell.execute_reply.started":"2024-11-11T16:03:34.347279Z","shell.execute_reply":"2024-11-11T16:03:34.352372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef calculate_fertility_score(tokenizer, data_list):\n    \"\"\"\n    Calculate the overall fertility score for the entire dataset based on the tokenizer output.\n\n    Args:\n    - tokenizer: The trained tokenizer object.\n    - data_list: A list of sentences for which fertility scores are calculated.\n\n    Returns:\n    - A single fertility score for the entire dataset.\n    \"\"\"\n    total_word_count = 0\n    total_token_count = 0\n\n    # Calculate word and token counts for each sentence\n    for sentence in data_list:\n        # Tokenize the sentence\n        input_ids = tokenizer.encode(sentence)\n        word_count = len(sentence.split())\n        token_count = len(input_ids)\n\n        total_word_count += word_count\n        total_token_count += token_count\n\n    # Calculate the overall fertility score\n    fertility_score = total_token_count / total_word_count if total_word_count > 0 else 0\n    return fertility_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:03:34.904318Z","iopub.execute_input":"2024-11-11T16:03:34.905306Z","iopub.status.idle":"2024-11-11T16:03:34.912896Z","shell.execute_reply.started":"2024-11-11T16:03:34.905253Z","shell.execute_reply":"2024-11-11T16:03:34.911536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the input folder containing the CSV files\ninput_folder = '/kaggle/input/hindi-dataset-10k-files/'  # Folder containing Hindi .csv files\n\n\noutput_file = \"fertility_score_BertWordPieceTokenizer.csv\"  # Output file for fertility scores\n\n# Collect data from all CSV files in the input folder\nhindi_data = []\nerr_cnt = 0\nfor filename in tqdm(os.listdir(input_folder), desc=\"Processing files\"):\n    try:\n        if filename.endswith(\".csv\"):\n            file_path = os.path.join(input_folder, filename)\n            # Process each file and append valid sentences\n            file_data = preprocess_text(file_path)\n            if file_data:  # Only extend if the file has valid sentences\n                hindi_data.extend(file_data)\n    except Exception as e:\n        err_cnt+=1\n\nprint('error occured in files: ',err_cnt)\n\nprint(f\"Total sentences for training: {len(hindi_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:03:35.826718Z","iopub.execute_input":"2024-11-11T16:03:35.827217Z","iopub.status.idle":"2024-11-11T16:04:20.447914Z","shell.execute_reply.started":"2024-11-11T16:03:35.827173Z","shell.execute_reply":"2024-11-11T16:04:20.446584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the tokenizer on the collected data\nif hindi_data:\n    model_name = \"BertWordPieceTokenizer\"\n    train_tokenizer(hindi_data, vocab_size=32000, model_name=model_name)\n\n    # Load the trained tokenizer for testing\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Calculate the overall fertility score\n    fertility_score = calculate_fertility_score(tokenizer, hindi_data)\n    \n    # Save the fertility score to a CSV file\n    with open(output_file, \"w\", encoding='utf-8-sig') as f:\n        f.write(\"Fertility Score\\n\")\n        f.write(f\"{fertility_score}\\n\")\n    print(f\"Fertility score saved to '{output_file}'\")\n\n    # Display the fertility score\n    print(f\"Overall Fertility Score: {fertility_score}\")\nelse:\n    print(\"No valid data found to train the tokenizer.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:05:03.196688Z","iopub.execute_input":"2024-11-11T16:05:03.197215Z","iopub.status.idle":"2024-11-11T16:23:10.702640Z","shell.execute_reply.started":"2024-11-11T16:05:03.197168Z","shell.execute_reply":"2024-11-11T16:23:10.701432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
